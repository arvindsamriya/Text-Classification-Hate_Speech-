{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "amazon review.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arvindsamriya/Text-Classification-Sentiment-Analysis/blob/master/amazon_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Gzzenguv3_g",
        "outputId": "053f8837-a896-428e-c750-86455893de48"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7adczdCZumpr"
      },
      "source": [
        "# Description \n",
        "The objective of this project is to detect the racist and sexist comments from amazon reviews or does a review contain any hate speech in it,we will classify these reviews.\n",
        "We will use a Training sample to build a model and use that model on a test dataset to predict the labels on that dataset and find the accuracy of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK3CugY6umps"
      },
      "source": [
        "# Introduction\n",
        "In this whole process we will use the amazon review 'txt' file to build the system and in this file we have 10000 examples, We \n",
        "will follow the following components to get the result-\n",
        "#### -Dataset Preparation\n",
        "#### -Feature Extraction\n",
        "#### - Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK6NeHl0umps"
      },
      "source": [
        "##libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import model_selection,preprocessing,svm,metrics\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icaUjSxCumpt"
      },
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "GZcoP5Neumpt",
        "outputId": "d0337754-8697-4878-bb30-9412fa4b1bab"
      },
      "source": [
        "#We will convert the raw data into a meaning full and informative dataset\n",
        "review_data=pd.read_csv('/content/drive/MyDrive/Data Sets/amazon review/review.txt',sep=('\\n'),header=None)\n",
        "print(review_data)\n",
        "#review_data.iloc[:,0]=review_data.iloc[:,0].str.lower()\n",
        "review_data['label']=''\n",
        "review_data['Text']=''\n",
        "texts=[]\n",
        "for i in review_data.index:\n",
        "    body=str(review_data.iloc[i,0]).split()\n",
        "    review_data['label'][i]=body[0]\n",
        "    review_data['Text'][i]=\" \".join(body[1:])\n",
        "review_data.head()"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                      0\n",
            "0     __label__2 Stuning even for the non-gamer: Thi...\n",
            "1     __label__2 The best soundtrack ever to anythin...\n",
            "2     __label__2 Amazing!: This soundtrack is my fav...\n",
            "3     __label__2 Excellent Soundtrack: I truly like ...\n",
            "4     __label__2 Remember, Pull Your Jaw Off The Flo...\n",
            "...                                                 ...\n",
            "9995  __label__2 A revelation of life in small town ...\n",
            "9996  __label__2 Great biography of a very interesti...\n",
            "9997  __label__1 Interesting Subject; Poor Presentat...\n",
            "9998  __label__1 Don't buy: The box looked used and ...\n",
            "9999  __label__2 Beautiful Pen and Fast Delivery.: T...\n",
            "\n",
            "[10000 rows x 1 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>__label__2 Stuning even for the non-gamer: Thi...</td>\n",
              "      <td>__label__2</td>\n",
              "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>__label__2 The best soundtrack ever to anythin...</td>\n",
              "      <td>__label__2</td>\n",
              "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>__label__2 Amazing!: This soundtrack is my fav...</td>\n",
              "      <td>__label__2</td>\n",
              "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>__label__2 Excellent Soundtrack: I truly like ...</td>\n",
              "      <td>__label__2</td>\n",
              "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>__label__2 Remember, Pull Your Jaw Off The Flo...</td>\n",
              "      <td>__label__2</td>\n",
              "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  ...                                               Text\n",
              "0  __label__2 Stuning even for the non-gamer: Thi...  ...  Stuning even for the non-gamer: This sound tra...\n",
              "1  __label__2 The best soundtrack ever to anythin...  ...  The best soundtrack ever to anything.: I'm rea...\n",
              "2  __label__2 Amazing!: This soundtrack is my fav...  ...  Amazing!: This soundtrack is my favorite music...\n",
              "3  __label__2 Excellent Soundtrack: I truly like ...  ...  Excellent Soundtrack: I truly like this soundt...\n",
              "4  __label__2 Remember, Pull Your Jaw Off The Flo...  ...  Remember, Pull Your Jaw Off The Floor After He...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glZPlyihxSIx"
      },
      "source": [
        "## Cleaning Text\n",
        "- Removing Stopwords\n",
        "- removing puntuations\n",
        "- removing all words that contain numbers\n",
        "- lemmatizer\n",
        "- include only Verb,noun and Adjectives\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKv8v140yGVd",
        "outputId": "4fed8aa1-c2af-4ea0-ec55-c23e147b0fee"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "exclude = set(string.punctuation)\n",
        "stopword=set(stopwords.words('english'))\n",
        "def clean_text(doc):\n",
        "  text1=[i for i in nltk.word_tokenize(doc.lower()) if i not in stopword]\n",
        "  text2=[i for i in text1 if i not in exclude]\n",
        "  text3=[w for w in text2 if re.search('^\\D',w)]\n",
        "  text4=\" \".join([nltk.WordNetLemmatizer().lemmatize(i) for i in text3])\n",
        "  #text5=\" \".join([w for (w,pt) in nltk.p.os_tag(text4) if pt=='JJ' or pt=='NN' or pt=='JJR' or pt=='JJS']) \n",
        "  return text4\n",
        "clean_text=review_data['Text'].apply(lambda x:clean_text(x))\n",
        "clean_text\n",
        "#clean_text(\"Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\")"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       stuning even non-gamer sound track beautiful p...\n",
              "1       best soundtrack ever anything 'm reading lot r...\n",
              "2       amazing soundtrack favorite music time hand in...\n",
              "3       excellent soundtrack truly like soundtrack enj...\n",
              "4       remember pull jaw floor hearing 've played gam...\n",
              "                              ...                        \n",
              "9995    revelation life small town america early thoug...\n",
              "9996    great biography interesting journalist biograp...\n",
              "9997    interesting subject poor presentation 'd hard-...\n",
              "9998    n't buy box looked used obviously new tried co...\n",
              "9999    beautiful pen fast delivery pen shipped prompt...\n",
              "Name: Text, Length: 10000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgZIOmD6umpy"
      },
      "source": [
        "## Feature Extraction using TFidf Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1Id5gLFumpy"
      },
      "source": [
        "x_train,x_test,y_train,y_test= model_selection.train_test_split(clean_text,review_data['label'])\n",
        "#split the data into test and training datasets\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "y_train= encoder.fit_transform(y_train)\n",
        "y_test= encoder.fit_transform(y_test)\n",
        "#we transform y_train and y_test into the binary class according to the labels.\n",
        "tfidf=TfidfVectorizer(min_df=20,max_df=0.90,ngram_range=(1,2)).fit(x_train)\n",
        "#we have use stopwords like he,are,is etc these words don't bring any feature to our dataset.\n",
        "#min_df removes words which are only present in less than 10 documents in the whole x_train.\n",
        "#max_df removes words which are present in 99% of the x_train documents.\n",
        "x_train=tfidf.transform(x_train)\n",
        "x_test=tfidf.transform(x_test)\n",
        "#feature extraction is completed here.\n",
        "#tfidf.get_feature_names() "
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzyK0rxFumpy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "40c9d9d7-db0c-4735-b56c-95f65423ef13"
      },
      "source": [
        "tfidf_df=pd.DataFrame(x_train.todense(),columns=tfidf.get_feature_names())\n",
        "tfidf_df.head()"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>absolute</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>ac</th>\n",
              "      <th>accent</th>\n",
              "      <th>access</th>\n",
              "      <th>account</th>\n",
              "      <th>accurate</th>\n",
              "      <th>across</th>\n",
              "      <th>act</th>\n",
              "      <th>acting</th>\n",
              "      <th>action</th>\n",
              "      <th>action movie</th>\n",
              "      <th>actor</th>\n",
              "      <th>actress</th>\n",
              "      <th>actual</th>\n",
              "      <th>actually</th>\n",
              "      <th>adam</th>\n",
              "      <th>adam sandler</th>\n",
              "      <th>adapter</th>\n",
              "      <th>add</th>\n",
              "      <th>added</th>\n",
              "      <th>addition</th>\n",
              "      <th>admit</th>\n",
              "      <th>adult</th>\n",
              "      <th>advanced</th>\n",
              "      <th>adventure</th>\n",
              "      <th>advertised</th>\n",
              "      <th>advice</th>\n",
              "      <th>affair</th>\n",
              "      <th>afraid</th>\n",
              "      <th>again</th>\n",
              "      <th>age</th>\n",
              "      <th>ago</th>\n",
              "      <th>agree</th>\n",
              "      <th>ahead</th>\n",
              "      <th>ai</th>\n",
              "      <th>air</th>\n",
              "      <th>air mattress</th>\n",
              "      <th>...</th>\n",
              "      <th>would given</th>\n",
              "      <th>would good</th>\n",
              "      <th>would highly</th>\n",
              "      <th>would like</th>\n",
              "      <th>would make</th>\n",
              "      <th>would never</th>\n",
              "      <th>would nice</th>\n",
              "      <th>would read</th>\n",
              "      <th>would recommend</th>\n",
              "      <th>would work</th>\n",
              "      <th>wow</th>\n",
              "      <th>write</th>\n",
              "      <th>writer</th>\n",
              "      <th>writes</th>\n",
              "      <th>writing</th>\n",
              "      <th>writing style</th>\n",
              "      <th>written</th>\n",
              "      <th>written book</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wrote</th>\n",
              "      <th>xp</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>year ago</th>\n",
              "      <th>year later</th>\n",
              "      <th>year old</th>\n",
              "      <th>year solitude</th>\n",
              "      <th>yes</th>\n",
              "      <th>yet</th>\n",
              "      <th>york</th>\n",
              "      <th>you</th>\n",
              "      <th>young</th>\n",
              "      <th>young woman</th>\n",
              "      <th>younger</th>\n",
              "      <th>yr</th>\n",
              "      <th>zen</th>\n",
              "      <th>zero</th>\n",
              "      <th>zero star</th>\n",
              "      <th>zeta</th>\n",
              "      <th>zeta jones</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.093033</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14612</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.331118</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 2409 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   ability  able  absolute  absolutely  ...  zero  zero star  zeta  zeta jones\n",
              "0      0.0   0.0       0.0         0.0  ...   0.0        0.0   0.0         0.0\n",
              "1      0.0   0.0       0.0         0.0  ...   0.0        0.0   0.0         0.0\n",
              "2      0.0   0.0       0.0         0.0  ...   0.0        0.0   0.0         0.0\n",
              "3      0.0   0.0       0.0         0.0  ...   0.0        0.0   0.0         0.0\n",
              "4      0.0   0.0       0.0         0.0  ...   0.0        0.0   0.0         0.0\n",
              "\n",
              "[5 rows x 2409 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBlSHg7bumpy"
      },
      "source": [
        ""
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyfRI7i9umpy"
      },
      "source": [
        "## Different ML Models and Model Selection\n",
        "We have extracted features and labels dataset, and now we will apply different ML models on the training dataset and calculate the accuracy for each of them.\n",
        "- Naive Bayes\n",
        "- Logistic Regression\n",
        "- Support Vector Machine(SVM)\n",
        "- Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0etPR8r4umpy"
      },
      "source": [
        "#Naive Bayes\n",
        "gridval={'alpha':[0.001,.01,.1,1,10]}\n",
        "model=MultinomialNB()\n",
        "grid=GridSearchCV(model,param_grid=gridval,scoring='accuracy')\n",
        "grid.fit(x_train,y_train)\n",
        "pred= grid.predict(x_test)\n",
        "accuracy_mnb=accuracy_score(y_test,pred)*100"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRtrpGHmumpy"
      },
      "source": [
        "#Logistic Regression\n",
        "gridval={'C':[0.0001,0.001,.01,.1,1,10]}\n",
        "model=LogisticRegression()\n",
        "grid=GridSearchCV(model,param_grid=gridval,scoring='accuracy')\n",
        "grid.fit(x_train,y_train)\n",
        "pred= grid.predict(x_test)\n",
        "accuracy_lrg=accuracy_score(y_test,pred)*100"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBeMGvweumpy"
      },
      "source": [
        "#Support Vector Machine (SVM)\n",
        "#gridval={'gamma':[0.0001,0.001,.01,.1,1,10],'C':[0.0001,0.001,.01,.1,1,10]}\n",
        "model=SVC(kernel='rbf')\n",
        "#grid=GridSearchCV(model,param_grid=gridval,scoring='accuracy')\n",
        "#grid.fit(x_train,y_train)\n",
        "#pred= grid.predict(x_test)\n",
        "model.fit(x_train,y_train)\n",
        "pred=model.predict(x_test)\n",
        "accuracy_svm=accuracy_score(y_test,pred)*100\n"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpSAtWclumpy"
      },
      "source": [
        "# Random Forest Model(RFM)\n",
        "model=RandomForestClassifier()\n",
        "#gridval={'n_estimators':[10,100,200,300,400,500]}\n",
        "#grid=GridSearchCV(model,param_grid=gridval,scoring='accuracy')\n",
        "#grid.fit(x_train,y_train)\n",
        "model.fit(x_train,y_train)\n",
        "#pred= grid.predict(x_test)\n",
        "pred=model.predict(x_test)\n",
        "accuracy_rfm=accuracy_score(y_test,pred)*100"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-khDnrXumpy",
        "outputId": "5189ed8a-7ef6-49eb-81f8-ac2022ab7371"
      },
      "source": [
        "#Result\n",
        "#Now we have accuracy of different models-\n",
        "print('Accuracies of Different Models\\nNaive Bayer: {}\\nLogistic Regression: {}\\nSupport Vector Machine(SVM):{}\\nRandom Forest Model: {}\\n '.format(accuracy_mnb,accuracy_lrg,accuracy_svm,accuracy_rfm))"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracies of Different Models\n",
            "Naive Bayer: 85.28\n",
            "Logistic Regression: 86.2\n",
            "Support Vector Machine(SVM):86.04\n",
            "Random Forest Model: 83.91999999999999\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmc77NNiumpz"
      },
      "source": [
        ""
      ],
      "execution_count": 165,
      "outputs": []
    }
  ]
}